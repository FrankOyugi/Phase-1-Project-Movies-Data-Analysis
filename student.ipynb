{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Frank Oyugi\n",
    "* Student pace: self paced / part time / full time : full time\n",
    "* Scheduled project review date/time: 22/03/2024\n",
    "* Instructor name: Maryann Mwikali\n",
    "* Blog post URL:https://github.com/FrankOyugi/Phase-1-Project-Movies-Data-Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title\n",
    "A Data Analysis Project  \n",
    "Author:Frank Oyugi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this project,we will be using Exploratory Data Analysis(EDA) to analyze five sets of datasets and use our findings to provide recommendations to 'Microsoft',a company trying to get into the film-making industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "Microsoft have decided to create a new movie studio,but they are new to the film-making business.We will be exploring what types of films are doing the best at box office then using those findings to provide insights to the movie studio and help decide what types of films to create.The data questions I plan to answer to solve this problem include:  \n",
    "1.What movies have been the highest-grossing each year?  \n",
    "2.What are the trends in box office revenue over time?  \n",
    "3.How are different movie genres performing?  \n",
    "4.How do gender preferences vary across demographic groups?  \n",
    "5.How do film ratings correlate with box office performance?  \n",
    "6.Who are the major competitors in the film-making industry,and what type of films do they produce?  \n",
    "7.What is happening with the highly popular streaming platforms?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project,we will be analyzing 5 datasets which contain various information that includes:  \n",
    "1.Movies  \n",
    "2.The movie genres  \n",
    "3.The movies ratings  \n",
    "4.The target audiences  \n",
    "5.Box office performance  \n",
    "These dataset names are as follows:  \n",
    "1.Box Office Mojo  \n",
    "2.IMDB  \n",
    "3.Rotten Tomatoes  \n",
    "4.The MovieDB  \n",
    "5.The Numbers \n",
    "\n",
    "Let's perform some data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - remember to use markdown cells for comments as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lenovo\\\\OneDrive\\\\Documents\\\\Flatiron\\\\Projects\\\\Phase-1-Project-Movies-Data-Analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing working directory to allow extracting information from the database without having to commit the entire project\n",
    "\n",
    "#os.chdir(r'C:\\Users\\lenovo\\OneDrive\\Documents\\Flatiron\\zippedData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM movie_basics;': no such table: movie_basics",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1680\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1681\u001b[1;33m             \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1682\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: movie_basics",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a3e415735c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Return first 5 records of 'movie_basics' and 'movie_ratings' columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmovie_basics_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM movie_basics;\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mmovie_ratings_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM movie_ratings;\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmovie_ratings_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmovie_basics_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas_sql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         return pandas_sql.read_query(\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m             \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM movie_basics;': no such table: movie_basics"
     ]
    }
   ],
   "source": [
    "#Open your db file\n",
    "\n",
    "#Open 'im.db' database\n",
    "conn = sqlite3.connect('im.db')\n",
    "\n",
    "#Return list of every table in the db\n",
    "table_names=pd.read_sql(\"\"\"SELECT name FROM sqlite_master WHERE type = 'table';\"\"\",conn)\n",
    "\n",
    "#Return first 5 records of 'movie_basics' and 'movie_ratings' columns\n",
    "movie_basics_columns = pd.read_sql(\"SELECT * FROM movie_basics;\",conn)\n",
    "movie_ratings_columns = pd.read_sql(\"SELECT * FROM movie_ratings;\",conn)\n",
    "movie_ratings_columns.head(),movie_basics_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('tn.movie_budgets.csv.gz')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('bom.movie_gross.csv.gz')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('tmdb.movies.csv.gz')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('rt.movie_info.tsv.gz', sep='\\t')\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('rt.reviews.tsv.gz', sep='\\t', encoding='unicode_escape')\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation  \n",
    "Correlation is used to measure the strength and the direction of the relationaship between two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between production_budget and domestic_gross\n",
    "\n",
    "budgets_df = pd.read_csv('tn.movie_budgets.csv.gz')\n",
    "\n",
    "budgets_df['production_budget'] = pd.to_numeric(budgets_df['production_budget'], errors='coerce')\n",
    "budgets_df['domestic_gross'] = pd.to_numeric(budgets_df['domestic_gross'], errors='coerce')\n",
    "\n",
    "budget_corr = budgets_df['production_budget'].corr(budgets_df['domestic_gross'])\n",
    "\n",
    "print(\"Correlation between 'production_budget' and 'domestic_gross':\", budget_corr)\n",
    "\n",
    "#The correlation is \"Nan\" because some cells have nan values and they have not been cleaned yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measures Of Central Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum average rating and genre\n",
    "query_1 = \"SELECT * FROM movie_basics\"\n",
    "query_2 = \"SELECT * FROM movie_ratings\"\n",
    "basics_df = pd.read_sql_query(query_1, conn)\n",
    "ratings_df = pd.read_sql_query(query_2, conn)\n",
    "merge_df = pd.merge(basics_df,ratings_df, on='movie_id')\n",
    "\n",
    "highest_rated_movie = merge_df.loc[merge_df['averagerating'].idxmax()]\n",
    "highest_rated_movie_title = highest_rated_movie['primary_title']\n",
    "highest_rated_movie_genre = highest_rated_movie['genres']\n",
    "#Minimum average rating and genre\n",
    "lowest_rated_movie = merge_df.loc[merge_df['averagerating'].idxmin()]\n",
    "lowest_rated_movie_title = lowest_rated_movie['primary_title']\n",
    "lowest_rated_movie_genre = lowest_rated_movie['genres']\n",
    "#Print results\n",
    "print('Movie with highest average rating:',highest_rated_movie_title,'.''Genre:',highest_rated_movie_genre)\n",
    "print('Movie with lowest average rating:',lowest_rated_movie_title,'.''Genre:',lowest_rated_movie_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Production Budget Per Year\n",
    "df1['release_date'] = pd.to_datetime(df1['release_date'])\n",
    "df1['release_year'] = df1['release_date'].dt.year\n",
    "df1['production_budget'] = df1['production_budget'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "mean_budget = df1.groupby('release_year')['production_budget'].mean()\n",
    "mean_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate no. of movies released per year\n",
    "released_year_numbers = df1['release_year'].value_counts()\n",
    "print(released_year_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Director with highest number of movies\n",
    "\n",
    "query_3 = \"SELECT * FROM directors\"\n",
    "query_4 = \"SELECT * FROM persons\"\n",
    "directors_df = pd.read_sql_query(query_3, conn)\n",
    "persons_df = pd.read_sql_query(query_4, conn)\n",
    "merged_df2 = pd.merge(directors_df,persons_df, on='person_id')\n",
    "\n",
    "director_counts = merged_df2['person_id'].value_counts()\n",
    "\n",
    "most_frequent_id = director_counts.idxmax()\n",
    "most_frequent_name = merged_df2.loc[merged_df2['person_id'] == most_frequent_id, 'primary_name'].iloc[0]\n",
    "\n",
    "#Dirctor with lowest number of movies\n",
    "\n",
    "least_frequent_id = director_counts.idxmin()\n",
    "least_frequent_name = merged_df2.loc[merged_df2['person_id'] == least_frequent_id, 'primary_name'].iloc[0]\n",
    "\n",
    "#Print\n",
    "\n",
    "print(\"The person who has directed the most movies is:\", most_frequent_name)\n",
    "print(\"The person who has directed the least movies is:\", least_frequent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writer with highest number of movies\n",
    "\n",
    "query_5 = \"SELECT * FROM writers\"\n",
    "\n",
    "writers_df = pd.read_sql_query(query_3, conn)\n",
    "\n",
    "merged_df3 = pd.merge(writers_df,persons_df, on='person_id')\n",
    "\n",
    "writer_counts = merged_df3['person_id'].value_counts()\n",
    "\n",
    "most_frequent_id_writer = writer_counts.idxmax()\n",
    "most_frequent_name_writer = merged_df3.loc[merged_df3['person_id'] == most_frequent_id_writer, 'primary_name'].iloc[0]\n",
    "\n",
    "#Writer with lowest number of movies\n",
    "\n",
    "least_frequent_id_writer = writer_counts.idxmin()\n",
    "least_frequent_name = merged_df3.loc[merged_df3['person_id'] == least_frequent_id_writer, 'primary_name'].iloc[0]\n",
    "\n",
    "#Print\n",
    "\n",
    "print(\"The person who has written the most movies is:\", most_frequent_name)\n",
    "print(\"The person who has written the least movies is:\", least_frequent_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning  \n",
    "This is the process of dealing with numerical probelms and inconsistencies in your data.Some data cleaning tasks include:  \n",
    "1.Cleaning empty cells.  \n",
    "2.Cleaning wrong formats.  \n",
    "3.Cleaning wrong data.  \n",
    "4.Removing duplicates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function For Removing Duplicates from csv files\n",
    "\n",
    "def find_duplicates(dataframe):\n",
    "    former_duplicates = dataframe.duplicated().sum() #Initial Duplicates\n",
    "    if former_duplicates > 0:\n",
    "        dataframe.drop_duplicates(inplace=True) #Removing Duplicates\n",
    "        latter_duplicates = dataframe.duplicated().sum() #Updated Duplicates\n",
    "        print(\"Duplicates removed. Number of duplicate rows before:\", former_duplicates, \", Number of duplicate rows after:\", latter_duplicates)\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates In Movie_Budget File\n",
    "\n",
    "df1 = find_duplicates(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movie_gross file\n",
    "\n",
    "df2 = find_duplicates(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movies file\n",
    "\n",
    "df3 = find_duplicates(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movie_info file\n",
    "\n",
    "df4 = find_duplicates(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in reviews file\n",
    "\n",
    "#There were 9 duplicates but I ran the code more than once\n",
    "\n",
    "find_duplicates(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for removing duplicates in database tables\n",
    "\n",
    "def find_duplicates_db(conn,table_name):\n",
    "    query = f\"SELECT * FROM {table_name};\"\n",
    "    duplicate_df = pd.read_sql_query(query, conn)\n",
    "    cleaned_df = duplicate_df.drop_duplicates()\n",
    "    cleaned_df.to_sql(name=table_name, con=conn, if_exists='replace', index=False)\n",
    "    print(\"Duplicates Removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in directors table)\n",
    "\n",
    "find_duplicates_db(conn,'directors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in writers table)\n",
    "\n",
    "find_duplicates_db(conn,'writers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in principals table)\n",
    "\n",
    "find_duplicates_db(conn,'principals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in persons table)\n",
    "\n",
    "find_duplicates_db(conn,'persons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in known_for table)\n",
    "\n",
    "find_duplicates_db(conn,'known_for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movie_ratings table)\n",
    "\n",
    "find_duplicates_db(conn,'movie_ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movie_akas table)\n",
    "\n",
    "find_duplicates_db(conn,'movie_akas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates in movie_basics table)\n",
    "\n",
    "find_duplicates_db(conn,'movie_basics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing 'currency','studio'and 'box_office' columns in 'movie_info table because of Nan values'\n",
    "df4 = pd.read_csv('rt.movie_info.tsv.gz', sep='\\t')\n",
    "\n",
    "columns_to_drop_movie_info = ['currency', 'studio','box_office']\n",
    "new_df4 = df4.drop(columns=columns_to_drop_movie_info, inplace=False)\n",
    "\n",
    "print(new_df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing 'rating' and 'top_critic' columns from 'reviews' file because of Nan values\n",
    "df5 = pd.read_csv('rt.reviews.tsv.gz', sep='\\t', encoding='unicode_escape')\n",
    "\n",
    "columns_to_drop_reviews =['rating','top_critic']\n",
    "new_df5 = df5.drop(columns=columns_to_drop_reviews, inplace=False)\n",
    "\n",
    "print(new_df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Nan values in movie_budgets file numerical columns with the mean of Non-Nan values\n",
    "#Replace 'Nan' with empty values\n",
    "df1 = pd.read_csv('tn.movie_budgets.csv.gz')\n",
    "\n",
    "df1.replace('NaN',np.nan,inplace=True)\n",
    "\n",
    "#Make all characters numeric\n",
    "\n",
    "df1['production_budget'] = df1['production_budget'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df1['domestic_gross'] = df1['domestic_gross'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df1['worldwide_gross'] = df1['worldwide_gross'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "#Find Mean\n",
    "\n",
    "mean_production_budget = df1['production_budget'].mean()\n",
    "mean_domestic_gross = df1['domestic_gross'].mean()\n",
    "mean_worldwide_gross = df1['worldwide_gross'].mean()\n",
    "\n",
    "#Replace empty values with mean\n",
    "\n",
    "new_df1 = df1.fillna({'production_budget': mean_production_budget,\n",
    "                     'domestic_gross': mean_domestic_gross,\n",
    "                     'worldwide_gross': mean_worldwide_gross}, inplace=False)\n",
    "#Print\n",
    "\n",
    "print(new_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replacing Nan values in movie_gross file numerical columns with the mean of Non-Nan values\n",
    "df2 = pd.read_csv('bom.movie_gross.csv.gz')\n",
    "\n",
    "#Replace 'Nan' with empty values\n",
    "\n",
    "df2.replace('NaN',np.nan,inplace=True)\n",
    "\n",
    "#Make in characters in foreign_gross column numeric\n",
    "\n",
    "df2['foreign_gross'] = df2['foreign_gross'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "#Find Mean\n",
    "\n",
    "mean_domestic_gross_2 = df2['domestic_gross'].mean()\n",
    "mean_foreign_gross = df2['foreign_gross'].mean()\n",
    "\n",
    "#Replace the empty values with mean\n",
    "\n",
    "new_df2 = df2.fillna({'domestic_gross': mean_domestic_gross_2,\n",
    "                     'foreign_gross': mean_foreign_gross}, inplace=False)\n",
    "\n",
    "print(new_df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations  \n",
    "These are graphical representations of data and information.They play a crucial role in understanding the patterns,trends and relatioships in data.Some of the common visualizations used un data analysis include:  \n",
    "1.Bar Plots.  \n",
    "2.Scatter Plots.  \n",
    "3.Histograms.  \n",
    "4.Line Plots.  \n",
    "5.Heat Maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap showing correlation relationship between production budget and domestic gross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(new_df1[['production_budget', 'domestic_gross']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap: Production Budget vs. Domestic Gross')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion.\n",
    "From the heat map above,we can conclude that there is a positive correlation relationship between production_budget and domestic_gross(0.69),which means when the production_budget increases,the domestic_gross also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap showing correlation relationship between domestic gross and worldwide gross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(new_df1[['domestic_gross', 'worldwide_gross']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap: Domestic Gross vs. Worldwide Gross')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion.\n",
    "From the heat map above,we can conclude that there is a  very strong positive correlation relationship between domestic_gross and worldwide_gross(0.94),which means when the production_budget increases,the domestic_gross will almost always also increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram showing frequency distribution of average movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6 = \"SELECT * FROM movie_ratings\"\n",
    "\n",
    "ratings_df = pd.read_sql_query(query_6, conn)\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.hist(ratings_df['averagerating'], bins=20, color='lightblue', edgecolor='black')  \n",
    "plt.title('Histogram showing distribution of average movie ratings')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion.  \n",
    "From the histogram above,we can conclude that most movies are rated at 6.5.This also shows that 6.5 is the mean average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram showing frequency distribution of movie runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_7 = \"SELECT * FROM movie_basics\"\n",
    "\n",
    "ratings_df = pd.read_sql_query(query_7, conn)\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.hist(ratings_df['runtime_minutes'], bins=5,range=(0,200), color='lightblue', edgecolor='black')  \n",
    "plt.title('Histogram showing distribution of movie runtimes')\n",
    "plt.xlabel('Runtime in minutes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion.  \n",
    "From the histogram above,we can conclude that most movies are 100 minutes long./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart Showing The Top 5 And Bottom 5 Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = pd.read_sql_query(query_7, conn)\n",
    "\n",
    "# Count the occurrences of each genre\n",
    "\n",
    "genre_counts = genres_df['genres'].value_counts()\n",
    "\n",
    "# Select top 5 and bottom 5 genres\n",
    "\n",
    "top_bottom_genres = pd.concat([genre_counts.head(5), genre_counts.tail(5)])\n",
    "\n",
    "# Plot the bar chart\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "top_bottom_genres.plot(kind='bar', color='skyblue', edgecolor='black') \n",
    "plt.title('Bar Chart Showing Top 5 and Bottom 5 Genres by Occurrences')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion.  \n",
    "From the bar chart above,we can conclude that the 5 most popular genres are documentary,drama,comedy,horror and comedy/drama,which the most popular one being documentary,which supports the theory that the highest rated movie is also a documentary as we had seen before durig data exploration.At the same time,we ca see that the 5 least popular documentaries are documentary/musical/mystery,drama/horror/short,action/animation/music,crime/family/horror ad fantasy/history/sci-fi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations.  \n",
    "From the results of data exploration and the visualizations that I've created,I would make the following recommendations to Microsoft:  \n",
    "1.Invest heavily in the production budget.The heat maps show that their is a positive correlation between production budget and domestic gross.  \n",
    "2.Focus on making documentaries and dramas.The histogram above shows that these two genres are the most popular ones.  \n",
    "3.Hire 'Omar Pasha' as your writer and director.He has contributed to the highest number of films in this dataset.  \n",
    "4.Avoid hiring 'Raphaelle Ayach' as your writer and director.He has contributed to the lowest number of films in this dataset.  \n",
    "5.make movies which are between 90 and 110 minutes long.The average runtime is 100 minutes.\n",
    "6.Do not make musicals and fantasies.They are the least popular genres.  \n",
    "7.Target movie ratings of 6.5 or higher.6.5 is the average movie rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
